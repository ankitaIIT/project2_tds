<html lang="en" data-bs-theme="light"><head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TDS 2025 Jan GA4 - Data Sourcing</title>
    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css" integrity="sha384-XGjxtQfXaH2tnPFa9x+ruJTuLE3Aa6LhHSWRr1XeTyhezb4abCG4ccI5AkVDxqC+" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.10.0/build/styles/default.min.css" integrity="sha384-4Y0nObtF3CbKnh+lpzmAVdAMtQXl+ganWiiv73RcGVdRdfVIya8Cao1C8ZsVRRDz" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.10.0/build/styles/github-dark.min.css">
    <script src="https://browser.sentry-cdn.com/8.55.0/bundle.tracing.replay.min.js" crossorigin="anonymous"></script><script src="https://js.sentry-cdn.com/f9b738b8a1bfb42581e666ab0a7dbb22.min.js" crossorigin="anonymous"></script>
    <style>
      body {
        padding-top: 64px;
      }
  
      body[data-status="admin"] #navbar {
        background-color: var(--bs-primary) !important;
  
        .navbar-brand::before {
          content: "[Admin] ";
        }
      }
  
      body[data-status="running"] #navbar {
        background-color: var(--bs-success) !important;
      }
  
      body[data-status="ended"] #navbar {
        background-color: var(--bs-danger) !important;
      }
  
      [id] {
        scroll-margin-top: 4rem;
      }
  
      textarea::-webkit-scrollbar {
        width: 8px;
      }
  
      textarea::-webkit-scrollbar-track {
        background: #333;
      }
  
      textarea::-webkit-scrollbar-thumb {
        background: #666;
        cursor: default;
      }
  
      textarea::-webkit-scrollbar-thumb:hover {
        background: #888;
      }
  
      /* Add a YouTube icon on top of all YouTube links with images */
      a[href*="youtu"]:has(img[src*="ytimg"]) {
        position: relative;
        display: inline-block;
      }
      a[href*="youtu"]:has(img[src*="ytimg"])::after {
        content: "";
        position: absolute;
        left: 50%;
        top: 50%;
        width: 100px;
        height: 100px;
        background: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="red" class="bi bi-youtube" viewBox="0 0 16 16"><path d="M8.051 1.999h.089c.822.003 4.987.033 6.11.335a2.01 2.01 0 0 1 1.415 1.42c.101.38.172.883.22 1.402l.01.104.022.26.008.104c.065.914.073 1.77.074 1.957v.075c-.001.194-.01 1.108-.082 2.06l-.008.105-.009.104c-.05.572-.124 1.14-.235 1.558a2.01 2.01 0 0 1-1.415 1.42c-1.16.312-5.569.334-6.18.335h-.142c-.309 0-1.587-.006-2.927-.052l-.17-.006-.087-.004-.171-.007-.171-.007c-1.11-.049-2.167-.128-2.654-.26a2.01 2.01 0 0 1-1.415-1.419c-.111-.417-.185-.986-.235-1.558L.09 9.82l-.008-.104A31 31 0 0 1 0 7.68v-.123c.002-.215.01-.958.064-1.778l.007-.103.003-.052.008-.104.022-.26.01-.104c.048-.519.119-1.023.22-1.402a2.01 2.01 0 0 1 1.415-1.42c.487-.13 1.544-.21 2.654-.26l.17-.007.172-.006.086-.003.171-.007A100 100 0 0 1 7.858 2z"/><path d="M6.4 5.209v4.818l4.157-2.408z" fill="white"/></svg>') no-repeat center center;
        background-size: contain;
        transform: translate(-50%, -50%);
        pointer-events: none;
      }
  </style>
  <style id="googleidentityservice_button_styles">.qJTHM{-webkit-user-select:none;color:#202124;direction:ltr;-webkit-touch-callout:none;font-family:"Roboto-Regular",arial,sans-serif;-webkit-font-smoothing:antialiased;font-weight:400;margin:0;overflow:hidden;-webkit-text-size-adjust:100%}.ynRLnc{left:-9999px;position:absolute;top:-9999px}.L6cTce{display:none}.bltWBb{word-break:break-all}.hSRGPd{color:#1a73e8;cursor:pointer;font-weight:500;text-decoration:none}.Bz112c-W3lGp{height:16px;width:16px}.Bz112c-E3DyYd{height:20px;width:20px}.Bz112c-r9oPif{height:24px;width:24px}.Bz112c-uaxL4e{-webkit-border-radius:10px;border-radius:10px}.LgbsSe-Bz112c{display:block}.S9gUrf-YoZ4jf,.S9gUrf-YoZ4jf *{border:none;margin:0;padding:0}.fFW7wc-ibnC6b>.aZ2wEe>div{border-color:#4285f4}.P1ekSe-ZMv3u>div:nth-child(1){background-color:#1a73e8!important}.P1ekSe-ZMv3u>div:nth-child(2),.P1ekSe-ZMv3u>div:nth-child(3){background-image:linear-gradient(to right,rgba(255,255,255,.7),rgba(255,255,255,.7)),linear-gradient(to right,#1a73e8,#1a73e8)!important}.haAclf{display:inline-block}.nsm7Bb-HzV7m-LgbsSe{-webkit-border-radius:4px;border-radius:4px;-webkit-box-sizing:border-box;box-sizing:border-box;-webkit-transition:background-color .218s,border-color .218s;transition:background-color .218s,border-color .218s;-webkit-user-select:none;-webkit-appearance:none;background-color:#fff;background-image:none;border:1px solid #dadce0;color:#3c4043;cursor:pointer;font-family:"Google Sans",arial,sans-serif;font-size:14px;height:40px;letter-spacing:0.25px;outline:none;overflow:hidden;padding:0 12px;position:relative;text-align:center;vertical-align:middle;white-space:nowrap;width:auto}@media screen and (-ms-high-contrast:active){.nsm7Bb-HzV7m-LgbsSe{border:2px solid windowText;color:windowText}}.nsm7Bb-HzV7m-LgbsSe.pSzOP-SxQuSe{font-size:14px;height:32px;letter-spacing:0.25px;padding:0 10px}.nsm7Bb-HzV7m-LgbsSe.purZT-SxQuSe{font-size:11px;height:20px;letter-spacing:0.3px;padding:0 8px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe{padding:0;width:40px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe.pSzOP-SxQuSe{width:32px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe.purZT-SxQuSe{width:20px}.nsm7Bb-HzV7m-LgbsSe.JGcpL-RbRzK{-webkit-border-radius:20px;border-radius:20px}.nsm7Bb-HzV7m-LgbsSe.JGcpL-RbRzK.pSzOP-SxQuSe{-webkit-border-radius:16px;border-radius:16px}.nsm7Bb-HzV7m-LgbsSe.JGcpL-RbRzK.purZT-SxQuSe{-webkit-border-radius:10px;border-radius:10px}.nsm7Bb-HzV7m-LgbsSe.MFS4be-Ia7Qfc{border:none;color:#fff}.nsm7Bb-HzV7m-LgbsSe.MFS4be-v3pZbf-Ia7Qfc{background-color:#1a73e8}.nsm7Bb-HzV7m-LgbsSe.MFS4be-JaPV2b-Ia7Qfc{background-color:#202124;color:#e8eaed}.nsm7Bb-HzV7m-LgbsSe .nsm7Bb-HzV7m-LgbsSe-Bz112c{height:18px;margin-right:8px;min-width:18px;width:18px}.nsm7Bb-HzV7m-LgbsSe.pSzOP-SxQuSe .nsm7Bb-HzV7m-LgbsSe-Bz112c{height:14px;min-width:14px;width:14px}.nsm7Bb-HzV7m-LgbsSe.purZT-SxQuSe .nsm7Bb-HzV7m-LgbsSe-Bz112c{height:10px;min-width:10px;width:10px}.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-Bz112c{margin-left:8px;margin-right:-4px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe .nsm7Bb-HzV7m-LgbsSe-Bz112c{margin:0;padding:10px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe.pSzOP-SxQuSe .nsm7Bb-HzV7m-LgbsSe-Bz112c{padding:8px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe.purZT-SxQuSe .nsm7Bb-HzV7m-LgbsSe-Bz112c{padding:4px}.nsm7Bb-HzV7m-LgbsSe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-top-left-radius:3px;border-top-left-radius:3px;-webkit-border-bottom-left-radius:3px;border-bottom-left-radius:3px;display:-webkit-box;display:-webkit-flex;display:flex;justify-content:center;-webkit-align-items:center;align-items:center;background-color:#fff;height:36px;margin-left:-10px;margin-right:12px;min-width:36px;width:36px}.nsm7Bb-HzV7m-LgbsSe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf .nsm7Bb-HzV7m-LgbsSe-Bz112c,.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf .nsm7Bb-HzV7m-LgbsSe-Bz112c{margin:0;padding:0}.nsm7Bb-HzV7m-LgbsSe.pSzOP-SxQuSe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{height:28px;margin-left:-8px;margin-right:10px;min-width:28px;width:28px}.nsm7Bb-HzV7m-LgbsSe.purZT-SxQuSe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{height:16px;margin-left:-6px;margin-right:8px;min-width:16px;width:16px}.nsm7Bb-HzV7m-LgbsSe.Bz112c-LgbsSe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-radius:3px;border-radius:3px;margin-left:2px;margin-right:0;padding:0}.nsm7Bb-HzV7m-LgbsSe.JGcpL-RbRzK .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-radius:18px;border-radius:18px}.nsm7Bb-HzV7m-LgbsSe.pSzOP-SxQuSe.JGcpL-RbRzK .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-radius:14px;border-radius:14px}.nsm7Bb-HzV7m-LgbsSe.purZT-SxQuSe.JGcpL-RbRzK .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-radius:8px;border-radius:8px}.nsm7Bb-HzV7m-LgbsSe .nsm7Bb-HzV7m-LgbsSe-bN97Pc-sM5MNb{display:-webkit-box;display:-webkit-flex;display:flex;-webkit-align-items:center;align-items:center;-webkit-flex-direction:row;flex-direction:row;justify-content:space-between;-webkit-flex-wrap:nowrap;flex-wrap:nowrap;height:100%;position:relative;width:100%}.nsm7Bb-HzV7m-LgbsSe .oXtfBe-l4eHX{justify-content:center}.nsm7Bb-HzV7m-LgbsSe .nsm7Bb-HzV7m-LgbsSe-BPrWId{-webkit-flex-grow:1;flex-grow:1;font-family:"Google Sans",arial,sans-serif;font-weight:500;overflow:hidden;text-overflow:ellipsis;vertical-align:top}.nsm7Bb-HzV7m-LgbsSe.purZT-SxQuSe .nsm7Bb-HzV7m-LgbsSe-BPrWId{font-weight:300}.nsm7Bb-HzV7m-LgbsSe .oXtfBe-l4eHX .nsm7Bb-HzV7m-LgbsSe-BPrWId{-webkit-flex-grow:0;flex-grow:0}.nsm7Bb-HzV7m-LgbsSe .nsm7Bb-HzV7m-LgbsSe-MJoBVe{-webkit-transition:background-color .218s;transition:background-color .218s;bottom:0;left:0;position:absolute;right:0;top:0}.nsm7Bb-HzV7m-LgbsSe:hover,.nsm7Bb-HzV7m-LgbsSe:focus{-webkit-box-shadow:none;box-shadow:none;border-color:rgb(210,227,252);outline:none}.nsm7Bb-HzV7m-LgbsSe:hover .nsm7Bb-HzV7m-LgbsSe-MJoBVe,.nsm7Bb-HzV7m-LgbsSe:focus .nsm7Bb-HzV7m-LgbsSe-MJoBVe{background:rgba(66,133,244,.04)}.nsm7Bb-HzV7m-LgbsSe:active .nsm7Bb-HzV7m-LgbsSe-MJoBVe{background:rgba(66,133,244,.1)}.nsm7Bb-HzV7m-LgbsSe.MFS4be-Ia7Qfc:hover .nsm7Bb-HzV7m-LgbsSe-MJoBVe,.nsm7Bb-HzV7m-LgbsSe.MFS4be-Ia7Qfc:focus .nsm7Bb-HzV7m-LgbsSe-MJoBVe{background:rgba(255,255,255,.24)}.nsm7Bb-HzV7m-LgbsSe.MFS4be-Ia7Qfc:active .nsm7Bb-HzV7m-LgbsSe-MJoBVe{background:rgba(255,255,255,.32)}.nsm7Bb-HzV7m-LgbsSe .n1UuX-DkfjY{-webkit-border-radius:50%;border-radius:50%;display:-webkit-box;display:-webkit-flex;display:flex;height:20px;margin-left:-4px;margin-right:8px;min-width:20px;width:20px}.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-BPrWId{font-family:"Roboto";font-size:12px;text-align:left}.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-BPrWId .ssJRIf,.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-BPrWId .K4efff .fmcmS{overflow:hidden;text-overflow:ellipsis}.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-BPrWId .K4efff{display:-webkit-box;display:-webkit-flex;display:flex;-webkit-align-items:center;align-items:center;color:#5f6368;fill:#5f6368;font-size:11px;font-weight:400}.nsm7Bb-HzV7m-LgbsSe.jVeSEe.MFS4be-Ia7Qfc .nsm7Bb-HzV7m-LgbsSe-BPrWId .K4efff{color:#e8eaed;fill:#e8eaed}.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-BPrWId .K4efff .Bz112c{height:18px;margin:-3px -3px -3px 2px;min-width:18px;width:18px}.nsm7Bb-HzV7m-LgbsSe.jVeSEe .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-top-left-radius:0;border-top-left-radius:0;-webkit-border-bottom-left-radius:0;border-bottom-left-radius:0;-webkit-border-top-right-radius:3px;border-top-right-radius:3px;-webkit-border-bottom-right-radius:3px;border-bottom-right-radius:3px;margin-left:12px;margin-right:-10px}.nsm7Bb-HzV7m-LgbsSe.jVeSEe.JGcpL-RbRzK .nsm7Bb-HzV7m-LgbsSe-Bz112c-haAclf{-webkit-border-radius:18px;border-radius:18px}.L5Fo6c-sM5MNb{border:0;display:block;left:0;position:relative;top:0}.L5Fo6c-bF1uUb{-webkit-border-radius:4px;border-radius:4px;bottom:0;cursor:pointer;left:0;position:absolute;right:0;top:0}.L5Fo6c-bF1uUb:focus{border:none;outline:none}sentinel{}</style><script src="https://cdn.jsdelivr.net/npm/pdfkit@0.16.0/js/pdfkit.standalone.js"></script></head>
  
  <body data-status="ended">
  
    <nav class="navbar navbar-expand-lg fixed-top bg-body-tertiary" data-bs-theme="dark" id="navbar">
      <div class="container-fluid">
        <span class="navbar-brand" href=".">
          <span id="countdown">Ended at Sun, 9 Feb, 2025, 11:59 pm IST</span>
          <button id="score" type="button" class="btn btn-dark btn-sm mb-1 ms-2" disabled="">Score: 0</button>
          <button type="button" class="btn btn-outline-light btn-sm mb-1 ms-2 check-action" title="Check your score" disabled="">Check all</button>
          <button type="button" class="btn btn-outline-light btn-sm mb-1 ms-2 save-action" title="Save your progress" disabled="">Save</button>
        </span>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarSupportedContent">
          <div class="nav-item dropdown ms-auto" role="group" aria-label="Toggle dark mode" title="Toggle Dark Mode">
            <button class="dark-theme-toggle btn btn-outline-light dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Toggle theme (light)">
              <i class="bi bi-circle-half"></i> <span class="d-lg-none ms-2">Toggle theme</span>
            </button>
            <ul class="dropdown-menu dropdown-menu-end">
              <li><button class="dropdown-item active" data-bs-theme-value="light" aria-pressed="true"><i class="me-2 bi bi-sun-fill"></i> Light</button></li>
              <li><button class="dropdown-item" data-bs-theme-value="dark" aria-pressed="false"><i class="me-2 bi bi-moon-stars-fill"></i> Dark</button></li>
              <li><button class="dropdown-item" data-bs-theme-value="auto" aria-pressed="false"><i class="me-2 bi bi-circle-half"></i> Auto</button></li>
            </ul>
          </div>
        </div>
      </div>
    </nav>
  
    <main class="container">
      <section id="instructions">
      <h1 class="display-3 my-5">
        TDS 2025 Jan GA4 - Data Sourcing
      </h1>
  
      <h2 class="display-6 my-5">Instructions</h2>
      <ol>
        <li><strong>Learn what you need</strong>. Reading material is provided, but feel free to skip it if you can answer the question. (Or learn it, just for pleasure.)</li>
        <li><strong>Check answers regularly</strong> by pressing <kbd>Check</kbd>. It shows which answers are right or wrong. You can check multiple times.</li>
        <li><strong>Save regularly</strong> by pressing <kbd>Save</kbd>. You can save multiple times. Your last saved submission will be evaluated.</li>
        <li><strong>Reloading is OK</strong>. Your answers are saved in your browser (not server). Questions won't change except for randomized parameters.</li>
        <li><strong>Browser may struggle</strong>. If you face loading issues, turn off security restrictions or try a different browser.</li>
        <li><strong>Use anything</strong>. You can use any resources you want. The Internet, ChatGPT, friends, whatever. Use any libraries or frameworks you want.</li>
        <li><strong>It's hackable</strong>. It's possible to get the answer to <em>some</em> questions by hacking the code for this quiz. That's allowed.</li>
      </ol>
      <div class="alert alert-info d-flex align-items-center p-4 border-start border-info border-2 mt-5">
        <i class="bi bi-chat-square-text-fill fs-4 me-3"></i>
        <div>
          <strong>Have questions?</strong>
          <a href="https://discourse.onlinedegree.iitm.ac.in/t/ga4-data-sourcing-discussion-thread-tds-jan-2025/165959" target="_blank" class="alert-link ms-2">
            Join the discussion on Discourse
          </a>
        </div>
      </div>
  
      <div class="alert alert-success">
        <h4 class="alert-heading">Bonus marks for posting on Discourse</h4>
        <p>
          To encourage discussions, IITM BS students who reply to the discussion on
          <a class="alert-link" href="https://discourse.onlinedegree.iitm.ac.in/t/ga4-data-sourcing-discussion-thread-tds-jan-2025/165959" target="_blank">
            GA4 - Data Sourcing - Discussion Thread [TDS Jan 2025]
          </a>
          with a <strong>relevant</strong> question or reply will get 1 bonus mark on this graded assignment.
        </p>
      </div>
    </section>
  
      <section id="login" class="my-5"><!---->
          <div id="logged-in-email" class="mb-3">You are logged in as <strong><!--?lit$958752707$-->22f2001103@ds.study.iitm.ac.in</strong>.</div>
          <div class="mb-3">
            <button class="btn btn-sm btn-outline-danger">
              Logout
            </button>
          </div>
        </section>
  
      <section id="notification" class="my-5"><div class="alert alert-success" role="alert">
          <h4 class="alert-heading">Recent saves <small class="text-muted fs-6 fw-light">(most recent is your official score)</small></h4>
          <div class="d-flex align-items-center mt-2">
    <button class="btn btn-sm btn-outline-primary me-2 load-answers" data-bs-toggle="collapse" data-index="0">Reload</button>
    from 2/9/2025, 1:41:03 PM. Score: 7
  </div><div class="d-flex align-items-center mt-2">
    <button class="btn btn-sm btn-outline-primary me-2 load-answers" data-bs-toggle="collapse" data-index="1">Reload</button>
    from 2/9/2025, 1:38:22 PM. Score: 6
  </div><div class="d-flex align-items-center mt-2">
    <button class="btn btn-sm btn-outline-primary me-2 load-answers" data-bs-toggle="collapse" data-index="2">Reload</button>
    from 2/9/2025, 1:07:32 PM. Score: 6
  </div>
          </div></section>
  
      <section class="alert alert-info my-5 d-flex align-items-center d-none" id="loading-questions">
        <div class="spinner-border text-primary me-2" role="status"></div>
        <span>Loading questions...</span>
      </section>
  
      <form id="exam-form" class="needs-validation" novalidate="">
        <section id="questions" class="my-5"><!----><!----><h1 class="display-6">Questions</h1><!----><!----><ol class="mt-3">
          <!--?lit$958752707$--><!----><li><a href="#hg-google-sheets-importhtml"><!--?lit$958752707$-->Import HTML to Google Sheets</a> (<!--?lit$958752707$-->1 <!--?lit$958752707$-->mark)</li><!----><!----><li><a href="#hq-scrape-imdb-movies"><!--?lit$958752707$-->Scrape IMDb movies</a> (<!--?lit$958752707$-->1 <!--?lit$958752707$-->mark)</li><!----><!----><li><a href="#hq-wikipedia-outline"><!--?lit$958752707$-->Wikipedia Outline</a> (<!--?lit$958752707$-->1 <!--?lit$958752707$-->mark)</li><!----><!----><li><a href="#hq-bbc-weather-api"><!--?lit$958752707$-->Scrape the BBC Weather API</a> (<!--?lit$958752707$-->1 <!--?lit$958752707$-->mark)</li><!----><!----><li><a href="#hq-nominatim-api"><!--?lit$958752707$-->Find the bounding box of a city</a> (<!--?lit$958752707$-->1 <!--?lit$958752707$-->mark)</li><!----><!----><li><a href="#hq-hacker-news-search"><!--?lit$958752707$-->Search Hacker News</a> (<!--?lit$958752707$-->1 <!--?lit$958752707$-->mark)</li><!----><!----><li><a href="#hq-find-newest-github-user"><!--?lit$958752707$-->Find newest GitHub user</a> (<!--?lit$958752707$-->1 <!--?lit$958752707$-->mark)</li><!----><!----><li><a href="#hq-scheduled-github-actions"><!--?lit$958752707$-->Create a Scheduled GitHub Action</a> (<!--?lit$958752707$-->1 <!--?lit$958752707$-->mark)</li><!----><!----><li><a href="#hq-extract-tables-from-pdf"><!--?lit$958752707$-->Extract tables from PDF</a> (<!--?lit$958752707$-->1 <!--?lit$958752707$-->mark)</li><!----><!----><li><a href="#hq-pdf-to-markdown"><!--?lit$958752707$-->Convert a PDF to Markdown</a> (<!--?lit$958752707$-->1 <!--?lit$958752707$-->mark)</li><!---->
        </ol><!----><!---->
            <div class="card my-5" data-question="g-google-sheets-importhtml" id="hg-google-sheets-importhtml">
              <div class="card-header">
                <span class="badge text-bg-primary me-2"><!--?lit$958752707$-->1</span>
                <!--?lit$958752707$-->Import HTML to Google Sheets (<!--?lit$958752707$-->1 <!--?lit$958752707$-->mark)
              </div>
              <!--?lit$958752707$--><!----><div class="card-body border-bottom"><!--?lit$958752707$--><h2>Scraping with Excel</h2>
  <p><a href="https://youtu.be/OCl6UdpmzRQ" target="_blank" rel="noopener noreferrer"><img src="https://i.ytimg.com/vi_webp/OCl6UdpmzRQ/sddefault.webp" alt="Weather Scraping with Excel: Get the Data" class="img-fluid"></a></p>
  <p>You'll learn how to <a href="https://support.microsoft.com/en-au/office/import-data-from-the-web-b13eed81-33fe-410d-9247-1747269c28e4" target="_blank" rel="noopener noreferrer">import tables on the web using Excel</a>, covering:</p>
  <ul>
  <li><strong>Data Import from Web</strong>: Use the query feature in Excel to scrape data from websites.</li>
  <li><strong>Establishing Web Connections</strong>: Connect Excel to a web page using a URL.</li>
  <li><strong>Using Query Editor</strong>: Navigate the query editor to view and manage web data tables.</li>
  <li><strong>Loading Data</strong>: Load data from the web into Excel for further manipulation.</li>
  <li><strong>Data Transformation</strong>: Remove unnecessary columns and transform data as needed.</li>
  <li><strong>Applying Transformations</strong>: Track applied steps in the sequence for reproducibility.</li>
  <li><strong>Refreshing Data</strong>: Refresh the imported data to get the latest updates from the web.</li>
  </ul>
  <p>Here are links used in the video:</p>
  <ul>
  <li><a href="https://www.timeanddate.com/weather/india/chennai/ext" target="_blank" rel="noopener noreferrer">Chennai Weather Forecast</a></li>
  <li><a href="https://docs.google.com/spreadsheets/d/1a12ApZMD6CTiKRyO4RuauOO8IdYgACRL/view" target="_blank" rel="noopener noreferrer">Excel Scraping Workbook</a></li>
  </ul>
  <p>If you use Excel on Mac, the process is a bit different. See <a href="https://youtu.be/PuqVoVNWF20" target="_blank" rel="noopener noreferrer">Importing External Data Into Excel on Mac</a>.</p>
  </div><!----><!----><div class="card-body border-bottom"><!--?lit$958752707$--><h2>Scraping with Google Sheets</h2>
  <p><a href="https://youtu.be/eYQEk7XJM7s" target="_blank" rel="noopener noreferrer"><img src="https://i.ytimg.com/vi_webp/eYQEk7XJM7s/sddefault.webp" alt="Scraping with Google Sheets" class="img-fluid"></a></p>
  <p>You'll learn how to <a href="https://support.google.com/docs/answer/3093339?hl=en" target="_blank" rel="noopener noreferrer">import tables on the web using Google Sheets's <code>=IMPORTHTML()</code> formula</a>, covering:</p>
  <ul>
  <li><strong>Import HTML Formula</strong>: Use =IMPORTHTML(URL, "query", index) to fetch tables or lists from a web page.</li>
  <li><strong>Granting Access</strong>: Allow access for formulas to fetch data from external sources.</li>
  <li><strong>Checking Imported Data</strong>: Verify if the imported table matches the data on the web page.</li>
  <li><strong>Handling Errors</strong>: Understand common issues and how to resolve them.</li>
  <li><strong>Sorting Data</strong>: Copy imported data as values and sort it within Google Sheets.</li>
  <li><strong>Freezing Rows</strong>: Use frozen rows to maintain headers while sorting.</li>
  <li><strong>Live Formulas</strong>: Learn how web data updates automatically when the source changes.</li>
  <li><strong>Other Import Functions</strong>: IMPORTXML, IMPORTFEED, IMPORTRANGE, and IMPORTDATA for advanced data fetching options.</li>
  </ul>
  <p>Here are links used in the video:</p>
  <ul>
  <li><a href="https://docs.google.com/spreadsheets/d/1Qp_YTh1-hJHxjMWE_GofkvLIKgEdKxb6NFImpId3z9o/view" target="_blank" rel="noopener noreferrer">Google sheet used in the video</a></li>
  <li><a href="https://support.google.com/docs/answer/3093339" target="_blank" rel="noopener noreferrer"><code>IMPORTHTML()</code></a></li>
  <li><a href="https://support.google.com/docs/answer/3093342" target="_blank" rel="noopener noreferrer"><code>IMPORTXML()</code></a></li>
  <li><a href="https://en.wikipedia.org/wiki/Demographics_of_India" target="_blank" rel="noopener noreferrer">Demographics of India</a></li>
  <li><a href="https://en.wikipedia.org/wiki/List_of_highest-grossing_Indian_films" target="_blank" rel="noopener noreferrer">List of highest grossing Indian films</a></li>
  </ul>
  </div><!---->
              <div class="card-body"><!--?lit$958752707$-->
      <div class="mb-3">
        <h2>Sports Analytics for CricketPro</h2>
        <p>
          <strong>CricketPro Insights</strong> is a leading sports analytics firm specializing in providing in-depth
          statistical analysis and insights for cricket teams, coaches, and enthusiasts. Leveraging data from prominent
          sources like ESPN Cricinfo, CricketPro offers actionable intelligence that helps teams optimize player
          performance, strategize game plans, and engage with fans through detailed statistics and visualizations.
        </p>
        <p>
          In the competitive world of cricket, understanding player performance metrics is crucial for team selection,
          game strategy, and player development. However, manually extracting and analyzing batting statistics from
          extensive datasets spread across multiple web pages is time-consuming and prone to errors. To maintain their
          edge and deliver timely insights, CricketPro needs an efficient, automated solution to aggregate and analyze
          player performance data from ESPN Cricinfo's ODI (One Day International) batting statistics.
        </p>
        <p>
          CricketPro Insights has identified the need to automate the extraction and analysis of ODI batting statistics
          from ESPN Cricinfo to streamline their data processing workflow. The statistics are available on a paginated
          website, with each page containing a subset of player data. By automating this process, CricketPro aims to
          provide up-to-date insights on player performances, such as the number of duck outs (i.e. a score of zero),
          which are pivotal for team assessments and strategic planning.
        </p>
        <p>As part of this initiative, you are tasked with developing a solution that allows CricketPro analysts to:</p>
        <ol>
          <li>
            <strong>Navigate Paginated Data:</strong> Access specific pages of the ODI batting statistics based on varying
            requirements.
          </li>
          <li>
            <strong>Extract Relevant Data:</strong> Use Google Sheets' <code>IMPORTHTML</code> function to pull
            tabular data from ESPN Cricinfo.
          </li>
          <li>
            <strong>Analyze Performance Metrics:</strong> Count the number of ducks (where the player was out for 0 runs)
            each player has, aiding in performance evaluations.
          </li>
        </ol>
        <h2>Your Task</h2>
        <p>
          ESPN Cricinfo has
          <a href="https://stats.espncricinfo.com/stats/engine/stats/index.html?class=2;template=results;type=batting">ODI batting stats</a>
          for each batsman. The result is paginated across multiple pages. Count the number of ducks in page number
          <code><!--?lit$958752707$-->40</code>.
        </p>
        <ol>
          <li>
            <strong>Understanding the Data Source:</strong> ESPN Cricinfo's
            <a href="https://stats.espncricinfo.com/stats/engine/stats/index.html?class=2;template=results;type=batting">ODI batting statistics</a>
            are spread across multiple pages, each containing a table of player data. Go to page number
            <code><!--?lit$958752707$-->40</code>.
          </li>
          <li>
            <strong>Setting Up Google Sheets:</strong> Utilize Google Sheets' <code>IMPORTHTML</code> function to
            import table data from the URL for page number <code><!--?lit$958752707$-->40</code>.
          </li>
          <li>
            <strong>Data Extraction and Analysis:</strong> Pull the relevant table from the assigned page into Google
            Sheets. Locate the column that represents the number of ducks for each player. (It is titled "0".)
            Sum the values in the "0" column to determine the total number of ducks on that page.
          </li>
        </ol>
        <h2>Impact</h2>
        <p>By automating the extraction and analysis of cricket batting statistics, CricketPro Insights can:</p>
        <ul>
          <li>
            <strong>Enhance Analytical Efficiency:</strong> Reduce the time and effort required to manually gather and
            process player performance data.
          </li>
          <li>
            <strong>Provide Timely Insights:</strong> Deliver up-to-date statistical analyses that aid teams and coaches
            in making informed decisions.
          </li>
          <li>
            <strong>Scalability:</strong> Easily handle large volumes of data across multiple pages, ensuring
            comprehensive coverage of player performances.
          </li>
          <li>
            <strong>Data-Driven Strategies:</strong> Enable the development of data-driven strategies for player
            selection, training focus areas, and game planning.
          </li>
          <li>
            <strong>Client Satisfaction:</strong> Improve service offerings by providing accurate and insightful analytics
            that meet the specific needs of clients in the cricketing world.
          </li>
        </ul>
  
        <label class="form-label" for="g-google-sheets-importhtml">
          What is the total number of ducks across players on page number <code><!--?lit$958752707$-->40</code> of
          <a href="https://stats.espncricinfo.com/stats/engine/stats/index.html?class=2;template=results;type=batting">ESPN Cricinfo's ODI batting stats</a>?
        </label>
        <input class="form-control" type="number" required="" id="g-google-sheets-importhtml" name="g-google-sheets-importhtml" disabled=""><div class="valid-feedback mb-3 comment">Correct!</div>
              <div class="invalid-feedback mb-3 comment">Incorrect. Try again.</div>
      </div>
    </div>
              <div class="card-footer d-flex">
                <button type="button" class="btn btn-primary check-answer" data-question="g-google-sheets-importhtml" disabled="">Check</button>
              </div>
            </div>
          <!----><!---->
            <div class="card my-5" data-question="q-scrape-imdb-movies" id="hq-scrape-imdb-movies">
              <div class="card-header">
                <span class="badge text-bg-primary me-2"><!--?lit$958752707$-->2</span>
                <!--?lit$958752707$-->Scrape IMDb movies (<!--?lit$958752707$-->1 <!--?lit$958752707$-->mark)
              </div>
              <!--?lit$958752707$--><!----><div class="card-body border-bottom"><!--?lit$958752707$--><h2>Scraping IMDb with JavaScript</h2>
  <p><a href="https://youtu.be/YVIKZqZIcCo" target="_blank" rel="noopener noreferrer"><img src="https://i.ytimg.com/vi_webp/YVIKZqZIcCo/sddefault.webp" alt="Scraping the IMDb with Browser JavaScript" class="img-fluid"></a></p>
  <p>You'll learn how to scrape the <a href="https://www.imdb.com/chart/top" target="_blank" rel="noopener noreferrer">IMDb Top 250 movies</a> directly in the browser using JavaScript on the Chrome DevTools, covering:</p>
  <ul>
  <li><strong>Access Developer Tools</strong>: Use F12 or right-click &gt; Inspect to open developer tools in Chrome or Edge.</li>
  <li><strong>Inspect Elements</strong>: Identify and inspect HTML elements using the Elements tab.</li>
  <li><strong>Query Selectors</strong>: Use <code>document.querySelectorAll</code> and <code>document.querySelector</code> to find elements by CSS class.</li>
  <li><strong>Extract Text Content</strong>: Retrieve text content from elements using JavaScript.</li>
  <li><strong>Functional Programming</strong>: Apply <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/map" target="_blank" rel="noopener noreferrer">map</a>
  and <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Functions/Arrow_functions" target="_blank" rel="noopener noreferrer">arrow functions</a>
  for concise data processing.</li>
  <li><strong>Data Structuring</strong>: Collect and format data into an array of arrays.</li>
  <li><strong>Copying Data</strong>: Use the copy function to transfer data to the clipboard.</li>
  <li><strong>Convert to Spreadsheet</strong>: Use online tools to convert JSON data to CSV or Excel format.</li>
  <li><strong>Text Manipulation</strong>: Perform text splitting and cleaning in Excel for final data formatting.</li>
  </ul>
  <p>Here are links and references:</p>
  <ul>
  <li><a href="https://www.imdb.com/chart/top/" target="_blank" rel="noopener noreferrer">IMDB Top 250 movies</a></li>
  <li><a href="https://developer.chrome.com/docs/devtools/overview/" target="_blank" rel="noopener noreferrer">Learn about Chrome Devtools</a></li>
  </ul>
  </div><!---->
              <div class="card-body"><!--?lit$958752707$-->
      <div class="mb-3">
        <h2 id="-case-study-enhancing-content-curation-for-streamflix-streaming-service-">
          <strong>Content Curation for StreamFlix Streaming</strong>
        </h2>
        <p>
          <strong>StreamFlix</strong> is a rapidly growing streaming service aiming to provide a diverse and high-quality
          library of movies, TV shows, etc. to its subscribers. To maintain a competitive edge and ensure customer
          satisfaction, StreamFlix invests heavily in data-driven content curation. By analyzing movie ratings and other
          key metrics, the company seeks to identify films that align with subscriber preferences and emerging viewing
          trends.
        </p>
        <p>
          With millions of titles available on platforms like IMDb, manually sifting through titles to select suitable
          additions to StreamFlix's catalog is both time-consuming and inefficient. To streamline this process,
          StreamFlix's data analytics team requires an automated solution to extract and analyze movie data based on
          specific rating criteria.
        </p>
        <p>
          Develop a Python program that interacts with IMDb's dataset to extract detailed information about titles
          within a specified rating range. The extracted data should include the movie's unique ID, title, release
          year, and rating. This information will be used to inform content acquisition decisions, ensuring that
          StreamFlix consistently offers high-quality and well-received films to its audience.
        </p>
        <p>
          Imagine you are a data analyst at StreamFlix, responsible for expanding the platform's movie library. Your
          task is to identify titles that have received favorable ratings on IMDb, ensuring that the selected titles meet
          the company's quality standards and resonate with subscribers.
        </p>
        <p>To achieve this, you need to:</p>
        <ol>
          <li>
            <strong>Extract Data:</strong> Retrieve movie information from IMDb for all films that have a rating between
            <code><!--?lit$958752707$-->6</code> and <code><!--?lit$958752707$-->8</code>.
          </li>
          <li>
            <strong>Format Data:</strong> Structure the extracted information into a JSON format containing the following
            fields:
            <ul>
              <li><code>id</code>: The unique identifier for the movie on IMDb.</li>
              <li><code>title</code>: The official title of the movie.</li>
              <li><code>year</code>: The year the movie was released.</li>
              <li><code>rating</code>: The IMDb user rating for the movie.</li>
            </ul>
          </li>
        </ol>
        <h2>Your Task</h2>
        <ol>
          <li>
            <strong>Source:</strong> Utilize IMDb's advanced web search at
            <a href="https://www.imdb.com/search/title/">https://www.imdb.com/search/title/</a> to access movie data.
          </li>
          <li><strong>Filter:</strong> Filter all titles with a rating between <!--?lit$958752707$-->6 and <!--?lit$958752707$-->8.</li>
          <li>
            <p>
              <strong>Format:</strong> For up to the first 25 titles, extract the necessary details: ID, title, year, and
              rating. The ID of the movie is the part of the URL after <code>tt</code> in the <code>href</code> attribute.
              For example, <a href="https://www.imdb.com/title/tt10078772/"><code>tt10078772</code></a>. Organize the data into a JSON structure as follows:
            </p>
            <pre><code class="json hljs language-json" data-highlighted="yes"><span class="hljs-punctuation">[</span>
    <span class="hljs-punctuation">{</span> <span class="hljs-attr">"id"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"tt1234567"</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">"title"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"Movie 1"</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">"year"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"2021"</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">"rating"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"5.8"</span> <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span>
    <span class="hljs-punctuation">{</span> <span class="hljs-attr">"id"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"tt7654321"</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">"title"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"Movie 2"</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">"year"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"2019"</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">"rating"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"6.2"</span> <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span>
    <span class="hljs-comment">// ... more titles</span>
  <span class="hljs-punctuation">]</span></code></pre>
          </li>
          <li><strong>Submit:</strong> Submit the JSON data in the text box below.</li>
        </ol>
        <h2>Impact</h2>
        <p>
          By completing this assignment, you'll simulate a key component of a streaming service's content
          acquisition strategy. Your work will enable StreamFlix to make informed decisions about which titles to license,
          ensuring that their catalog remains both diverse and aligned with subscriber preferences. This, in turn,
          contributes to improved customer satisfaction and retention, driving the company's growth and success in a
          competitive market.
        </p>
        <label class="form-label" for="q-scrape-imdb-movies"> What is the JSON data? </label>
        <textarea class="form-control" rows="5" required="" id="q-scrape-imdb-movies" name="q-scrape-imdb-movies" disabled=""></textarea><div class="valid-feedback mb-3 comment">Correct!</div>
              <div class="invalid-feedback mb-3 comment">Incorrect. Try again.</div>
        <p class="text-muted">
          IMDb search results may differ by region. You may need to manually translate titles. Results may also change
          periodically. You may need to re-run your scraper code.
        </p>
      </div>
    </div>
              <div class="card-footer d-flex">
                <button type="button" class="btn btn-primary check-answer" data-question="q-scrape-imdb-movies" disabled="">Check</button>
              </div>
            </div>
          <!----><!---->
            <div class="card my-5" data-question="q-wikipedia-outline" id="hq-wikipedia-outline">
              <div class="card-header">
                <span class="badge text-bg-primary me-2"><!--?lit$958752707$-->3</span>
                <!--?lit$958752707$-->Wikipedia Outline (<!--?lit$958752707$-->1 <!--?lit$958752707$-->mark)
              </div>
              <!--?lit$958752707$--><!----><div class="card-body border-bottom"><!--?lit$958752707$--><h2>Scraping emarketer</h2>
  <p>In this live scraping session, we explore a real-life scenario where Straive had to scrape data from emarketer.com for a demo. This is a fairly realistic and representative way of how one might go about scraping a website.</p>
  <p><a href="https://youtu.be/ZzUsDE1XjhE" target="_blank" rel="noopener noreferrer"><img src="https://i.ytimg.com/vi_webp/ZzUsDE1XjhE/sddefault.webp" alt="Live scraping session" class="img-fluid"></a></p>
  <p>You'll learn:</p>
  <ul>
  <li><strong>Scraping</strong>: How to extract data from web pages, including constructing URLs, fetching page content, and parsing HTML using packages like <a href="https://lxml.de/" target="_blank" rel="noopener noreferrer"><code>lxml</code></a> and <a href="https://www.python-httpx.org/" target="_blank" rel="noopener noreferrer"><code>httpx</code></a>.</li>
  <li><strong>Caching</strong>: Implementing a caching strategy to avoid redundant data fetching for efficiency and reliability.</li>
  <li><strong>Error Handling and Debugging</strong>: Practical tips for troubleshooting, such as using liberal print statements, breakpoints for in-depth debugging, and the concept of "rubber duck debugging" to clarify problems.</li>
  <li><strong>LLMs</strong>: Benefits of Gemini / ChatGPT for code suggestions and troubleshooting.</li>
  <li><strong>Real-World Application</strong>: How quick proofs of concept to showcase capabilities to clients, emphasizing practice over theory.</li>
  </ul>
  </div><!---->
              <div class="card-body"><!--?lit$958752707$-->
      <div class="mb-3">
        <h2 id="a-country-information-api-for-globaledu">A Country Information API for GlobalEdu</h2>
        <p>
          <strong>GlobalEdu Platforms</strong> is a leading provider of educational technology solutions, specializing in
          creating interactive and informative content for students and educators worldwide. Their suite of products
          includes digital textbooks, educational apps, and online learning platforms that aim to make learning more
          engaging and accessible. To enhance their offerings, GlobalEdu Platforms seeks to integrate comprehensive
          country information into their educational tools, enabling users to access structured and easily navigable
          content about various nations.
        </p>
        <p>
          With the vast amount of information available on platforms like Wikipedia, manually curating and organizing
          country-specific data for educational purposes is both time-consuming and inefficient. GlobalEdu Platforms aims
          to automate this process to ensure that their educational materials are up-to-date, accurate, and
          well-structured. The key challenges they face include:
        </p>
        <ol>
          <li>
            <strong>Content Organization:</strong> Presenting information in a structured and hierarchical manner that
            aligns with educational standards.
          </li>
          <li>
            <strong>Scalability:</strong> Handling data for a large number of countries without manual intervention.
          </li>
          <li>
            <strong>Accessibility:</strong> Ensuring that the information is easily accessible from various applications
            and platforms used by educators and students.
          </li>
          <li>
            <strong>Interoperability:</strong> Allowing cross-origin requests to integrate the API seamlessly with
            different front-end applications.
          </li>
        </ol>
        <p>
          To address these challenges, GlobalEdu Platforms has decided to develop a web application that exposes a RESTful
          API. This API will allow their educational tools to fetch and display structured outlines of Wikipedia pages for
          any given country. The application needs to:
        </p>
        <ul>
          <li>Accept a country name as a query parameter.</li>
          <li>Fetch the corresponding Wikipedia page for that country.</li>
          <li>Extract all headings (H1 to H6) from the page.</li>
          <li>Generate a Markdown-formatted outline that reflects the hierarchical structure of the content.</li>
          <li>
            Enable Cross-Origin Resource Sharing (CORS) to allow GET requests from any origin, facilitating seamless
            integration with various educational platforms.
          </li>
        </ul>
        <h2 id="your-task">Your Task</h2>
        <p>
          Write a web application that exposes an API with a single query parameter: <code>?country=</code>. It should
          fetch the Wikipedia page of the country, extracts all headings (H1 to H6), and create a Markdown outline for the
          country. The outline should look like this:
        </p>
        <pre><code class="language-markdown hljs" data-highlighted="yes">
  <span class="hljs-section">## Contents</span>
  
  <span class="hljs-section"># Vanuatu</span>
  
  <span class="hljs-section">## Etymology</span>
  
  <span class="hljs-section">## History</span>
  
  <span class="hljs-section">### Prehistory</span>
  
  ...</code></pre>
        <ol>
          <li>
            <strong>API Development:</strong> Choose any web framework (e.g., FastAPI) to develop the web application.
            Create an API endpoint (e.g., <code>/api/outline</code>) that accepts a <code>country</code> query parameter.
          </li>
          <li>
            <strong>Fetching Wikipedia Content:</strong> Find out the Wikipedia URL of the country and fetch the
            page's HTML.
          </li>
          <li>
            <strong>Extracting Headings:</strong> Use an HTML parsing library (e.g., BeautifulSoup, lxml) to parse the
            fetched Wikipedia page. Extract all headings (H1 to H6) from the page, maintaining order.
          </li>
          <li>
            <strong>Generating Markdown Outline:</strong> Convert the extracted headings into a Markdown-formatted
            outline. Headings should begin with <code>#</code>.
          </li>
          <li>
            <strong>Enabling CORS:</strong> Configure the web application to include appropriate CORS headers, allowing
            GET requests from any origin.
          </li>
        </ol>
  
        <label class="form-label" for="q-wikipedia-outline"> What is the URL of your API endpoint? </label>
        <input class="form-control" type="url" required="" id="q-wikipedia-outline" name="q-wikipedia-outline" disabled=""><div class="valid-feedback mb-3 comment">Correct!</div>
              <div class="invalid-feedback mb-3 comment">Incorrect. Try again.</div>
        <p class="text-muted">
          We'll check by sending a request to this URL with <code>?country=...</code> passing different countries.
        </p>
      </div>
    </div>
              <div class="card-footer d-flex">
                <button type="button" class="btn btn-primary check-answer" data-question="q-wikipedia-outline" disabled="">Check</button>
              </div>
            </div>
          <!----><!---->
            <div class="card my-5" data-question="q-bbc-weather-api" id="hq-bbc-weather-api">
              <div class="card-header">
                <span class="badge text-bg-primary me-2"><!--?lit$958752707$-->4</span>
                <!--?lit$958752707$-->Scrape the BBC Weather API (<!--?lit$958752707$-->1 <!--?lit$958752707$-->mark)
              </div>
              <!--?lit$958752707$--><!----><div class="card-body border-bottom"><!--?lit$958752707$--><h2>BBC Weather location ID with Python</h2>
  <p><a href="https://youtu.be/IafLrvnamAw" target="_blank" rel="noopener noreferrer"><img src="https://i.ytimg.com/vi_webp/IafLrvnamAw/sddefault.webp" alt="BBC Weather location API with Python" class="img-fluid"></a></p>
  <p>You'll learn how to get the location ID of any city from the BBC Weather API -- as a precursor to scraping weather data -- covering:</p>
  <ul>
  <li><strong>Understanding API Calls</strong>: Learn how backend API calls work when searching for a city on the BBC weather website.</li>
  <li><strong>Inspecting Web Interactions</strong>: Use the browser's inspect element feature to track API calls and understand the network activity.</li>
  <li><strong>Extracting Location IDs</strong>: Identify and extract the location ID from the API response using Python.</li>
  <li><strong>Using Python Libraries</strong>: Import and use requests, json, and urlencode libraries to make API calls and process responses.</li>
  <li><strong>Constructing API URLs</strong>: Create structured API URLs dynamically with constant prefixes and query parameters using urlencode.</li>
  <li><strong>Building Functions</strong>: Develop a Python function that accepts a city name, constructs the API call, and returns the location ID.</li>
  </ul>
  <p>To open the browser Developer Tools on Chrome, Edge, or Firefox, you can:</p>
  <ul>
  <li>Right-click on the page and select "Inspect" to open the developer tools</li>
  <li>OR: Press <code>F12</code></li>
  <li>OR: Press <code>Ctrl+Shift+I</code> on Windows</li>
  <li>OR: Press <code>Cmd+Opt+I</code> on Mac</li>
  </ul>
  <p>Here are links and references:</p>
  <ul>
  <li><a href="https://colab.research.google.com/drive/1-iV-tbtRicKR_HXWeu4Hi5aXJCV3QdQp" target="_blank" rel="noopener noreferrer">BBC Location ID scraping - Notebook</a></li>
  <li><a href="https://www.bbc.com/weather/5380748" target="_blank" rel="noopener noreferrer">BBC Weather - Palo Alto (location ID: 5380748)</a></li>
  <li><a href="https://locator-service.api.bbci.co.uk/locations?api_key=AGbFAKx58hyjQScCXIYrxuEwJh2W2cmv&amp;stack=aws&amp;locale=en&amp;filter=international&amp;place-types=settlement%2Cairport%2Cdistrict&amp;order=importance&amp;s=los%20angeles&amp;a=true&amp;format=json" target="_blank" rel="noopener noreferrer">BBC Locator Service - Los Angeles</a></li>
  <li>Learn about the <a href="https://docs.python-requests.org/en/latest/user/quickstart/" target="_blank" rel="noopener noreferrer"><code>requests</code> package</a>. Watch <a href="https://youtu.be/tb8gHvYlCFs" target="_blank" rel="noopener noreferrer">Python Requests Tutorial: Request Web Pages, Download Images, POST Data, Read JSON, and More</a></li>
  </ul>
  <h2>BBC Weather data with Python</h2>
  <p><a href="https://youtu.be/Uc4DgQJDRoI" target="_blank" rel="noopener noreferrer"><img src="https://i.ytimg.com/vi_webp/Uc4DgQJDRoI/sddefault.webp" alt="Scrape BBC weather with Python" class="img-fluid"></a></p>
  <p>You'll learn how to scrape the live weather data of a city from the BBC Weather API, covering:</p>
  <ul>
  <li><strong>Introduction to Web Scraping</strong>: Understand the basics of web scraping and its legality.</li>
  <li><strong>Libraries Overview</strong>: Learn the importance of <a href="https://docs.python-requests.org/en/latest/user/quickstart/" target="_blank" rel="noopener noreferrer"><code>requests</code></a> and <a href="https://beautiful-soup-4.readthedocs.io/" target="_blank" rel="noopener noreferrer"><code>BeautifulSoup</code></a>.</li>
  <li><strong>Fetching HTML</strong>: Use <a href="https://docs.python-requests.org/en/latest/user/quickstart/" target="_blank" rel="noopener noreferrer"><code>requests</code></a> to fetch HTML content from a web page.</li>
  <li><strong>Parsing HTML</strong>: Utilize <a href="https://beautiful-soup-4.readthedocs.io/" target="_blank" rel="noopener noreferrer"><code>BeautifulSoup</code></a> to parse and navigate the HTML content.</li>
  <li><strong>Identifying Data</strong>: Inspect HTML elements to locate specific data (e.g., high and low temperatures).</li>
  <li><strong>Extracting Data</strong>: Extract relevant data using <a href="https://beautiful-soup-4.readthedocs.io/" target="_blank" rel="noopener noreferrer"><code>BeautifulSoup</code></a>'s <code>find_all()</code> function.</li>
  <li><strong>Data Cleanup</strong>: Clean extracted data to remove unwanted elements.</li>
  <li><strong>Post-Processing</strong>: Use regular expressions to split large strings into meaningful parts.</li>
  <li><strong>Data Structuring</strong>: Combine extracted data into a structured pandas DataFrame.</li>
  <li><strong>Handling Special Characters</strong>: Replace unwanted characters for better data manipulation.</li>
  <li><strong>Saving Data</strong>: Save the cleaned data into CSV and Excel formats.</li>
  </ul>
  <p>Here are links and references:</p>
  <ul>
  <li><a href="https://colab.research.google.com/drive/1-gkMzE-TKe3U_yh1v0NPn4TM687H2Hcf" target="_blank" rel="noopener noreferrer">BBC Weather scraping - Notebook</a></li>
  <li><a href="https://locator-service.api.bbci.co.uk/locations?api_key=AGbFAKx58hyjQScCXIYrxuEwJh2W2cmv&amp;stack=aws&amp;locale=en&amp;filter=international&amp;place-types=settlement%2Cairport%2Cdistrict&amp;order=importance&amp;s=mumbai&amp;a=true&amp;format=json" target="_blank" rel="noopener noreferrer">BBC Locator Service - Mumbai</a></li>
  <li><a href="https://www.bbc.com/weather/1275339" target="_blank" rel="noopener noreferrer">BBC Weather - Mumbai (location ID: 1275339)</a></li>
  <li><a href="https://weather-broker-cdn.api.bbci.co.uk/en/forecast/aggregated/1275339" target="_blank" rel="noopener noreferrer">BBC Weather API - Mumbai (location ID: 1275339)</a></li>
  <li>Learn about the <a href="https://docs.python.org/3/library/json.html" target="_blank" rel="noopener noreferrer"><code>json</code> package</a>. Watch <a href="https://youtu.be/9N6a-VLBa2I" target="_blank" rel="noopener noreferrer">Python Tutorial: Working with JSON Data using the json Module</a></li>
  <li>Learn about the <a href="https://beautiful-soup-4.readthedocs.io/" target="_blank" rel="noopener noreferrer"><code>BeautifulSoup</code> package</a>. Watch <a href="https://youtu.be/ng2o98k983k" target="_blank" rel="noopener noreferrer">Python Tutorial: Web Scraping with BeautifulSoup and Requests</a></li>
  <li>Learn about the <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html" target="_blank" rel="noopener noreferrer"><code>pandas</code> package</a>. Watch<ul>
  <li><a href="https://youtu.be/ZyhVh-qRZPA" target="_blank" rel="noopener noreferrer">Python Pandas Tutorial (Part 1): Getting Started with Data Analysis - Installation and Loading Data</a></li>
  <li><a href="https://youtu.be/zmdjNSmRXF4" target="_blank" rel="noopener noreferrer">Python Pandas Tutorial (Part 2): DataFrame and Series Basics - Selecting Rows and Columns</a></li>
  </ul>
  </li>
  <li>Learn about the <a href="https://docs.python.org/3/library/re.html" target="_blank" rel="noopener noreferrer"><code>re</code> package</a>. Watch <a href="https://youtu.be/K8L6KVGG-7o" target="_blank" rel="noopener noreferrer">Python Tutorial: re Module - How to Write and Match Regular Expressions (Regex)</a></li>
  <li>Learn about the <a href="https://docs.python.org/3/library/datetime.html" target="_blank" rel="noopener noreferrer"><code>datetime</code> package</a>. Watch <a href="https://youtu.be/eirjjyP2qcQ" target="_blank" rel="noopener noreferrer">Python Tutorial: Datetime Module - How to work with Dates, Times, Timedeltas, and Timezones</a></li>
  </ul>
  </div><!---->
              <div class="card-body"><!--?lit$958752707$-->
      <div class="mb-3">
        <h2>Weather Data Integration for AgroTech Insights</h2>
        <p>
          <strong>AgroTech Insights</strong> is a leading agricultural technology company that provides data-driven
          solutions to farmers and agribusinesses. By leveraging advanced analytics and real-time data, AgroTech helps
          optimize crop yields, manage resources efficiently, and mitigate risks associated with adverse weather
          conditions. Accurate and timely weather forecasts are crucial for making informed decisions in agricultural
          planning and management.
        </p>
        <p>
          Farmers and agribusinesses rely heavily on precise weather information to plan planting schedules, irrigation,
          harvesting, and protect crops from extreme weather events. However, accessing and processing weather data from
          multiple sources can be time-consuming and technically challenging. AgroTech Insights seeks to automate the
          extraction and transformation of weather data to provide seamless, actionable insights to its clients.
        </p>
        <p>
          AgroTech Insights has partnered with various stakeholders to enhance its weather forecasting capabilities. One
          of the key requirements is to integrate weather forecast data for specific regions to support crop management
          strategies. For this purpose, AgroTech utilizes the <strong>BBC Weather API</strong>, a reliable source of
          detailed weather information.
        </p>
        <h2>Your Task</h2>
        <p>As part of this initiative, you are tasked with developing a system that automates the following:</p>
        <ol>
          <li>
            <strong>API Integration and Data Retrieval:</strong> Use the BBC Weather API to fetch the weather forecast for
            <code><!--?lit$958752707$-->Santiago</code>. Send a GET request to the locator service to obtain the city's
            <code>locationId</code>. Include necessary query parameters such as API key, locale, filters, and search term
            (<code>city</code>).
          </li>
          <li>
            <strong>Weather Data Extraction:</strong> Retrieve the weather forecast data using the obtained
            <code>locationId</code>. Send a GET request to the weather broker API endpoint with the
            <code>locationId</code>.
          </li>
          <li>
            <strong>Data Transformation:</strong> Extract the <code>localDate</code> and
            <code>enhancedWeatherDescription</code> from each day's forecast. Iterate through the
            <code>forecasts</code> array in the API response and map each <code>localDate</code> to its corresponding
            <code>enhancedWeatherDescription</code>. Create a JSON object where each key is the <code>localDate</code> and
            the value is the <code>enhancedWeatherDescription</code>.
          </li>
        </ol>
        <p>The output would look like this:</p>
        <pre><code class="lang-json hljs language-json" data-highlighted="yes"><span class="hljs-punctuation">{</span>
    <span class="hljs-attr">"2025-01-01"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"Sunny with scattered clouds"</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">"2025-01-02"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"Partly cloudy with a chance of rain"</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">"2025-01-03"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"Overcast skies"</span><span class="hljs-punctuation">,</span>
    <span class="hljs-comment">// ... additional days</span>
  <span class="hljs-punctuation">}</span></code></pre>
  
        <label class="form-label" for="q-bbc-weather-api">
          What is the JSON weather forecast description for <code><!--?lit$958752707$-->Santiago</code>?
        </label>
        <textarea rows="6" class="form-control" required="" id="q-bbc-weather-api" name="q-bbc-weather-api" disabled=""></textarea><div class="valid-feedback mb-3 comment">Correct!</div>
              <div class="invalid-feedback mb-3 comment">Incorrect. Try again.</div>
      </div>
    </div>
              <div class="card-footer d-flex">
                <button type="button" class="btn btn-primary check-answer" data-question="q-bbc-weather-api" disabled="">Check</button>
              </div>
            </div>
          <!----><!---->
            <div class="card my-5" data-question="q-nominatim-api" id="hq-nominatim-api">
              <div class="card-header">
                <span class="badge text-bg-primary me-2"><!--?lit$958752707$-->5</span>
                <!--?lit$958752707$-->Find the bounding box of a city (<!--?lit$958752707$-->1 <!--?lit$958752707$-->mark)
              </div>
              <!--?lit$958752707$--><!----><div class="card-body border-bottom"><!--?lit$958752707$--><h2>Nominatim API with Python</h2>
  <p><a href="https://youtu.be/f0PZ-pphAXE" target="_blank" rel="noopener noreferrer"><img src="https://i.ytimg.com/vi_webp/f0PZ-pphAXE/sddefault.webp" alt="Nominatim Open Street Map with Python" class="img-fluid"></a></p>
  <p>You'll learn how to get the latitude and longitude of any city from the Nominatim API.</p>
  <ul>
  <li><strong>Introduction to Nominatim</strong>: Understand how Nominatim, from OpenStreetMap, works similarly to Google Maps for geocoding.</li>
  <li><strong>Installation and Import</strong>: Learn to install and import <a href="https://geopy.readthedocs.io/" target="_blank" rel="noopener noreferrer">geopy</a> and <a href="https://nominatim.org/" target="_blank" rel="noopener noreferrer">nominatim</a>.</li>
  <li><strong>Using the Locator</strong>: Create a locator object using Nominatim and set up a user agent.</li>
  <li><strong>Geocoding an Address</strong>: Use <code>locator.geocode</code> to input an address (e.g., Eiffel Tower) and fetch geocoded data.</li>
  <li><strong>Extracting Data</strong>: Access detailed information like latitude, longitude, bounding box, and accurate address from the JSON response.</li>
  <li><strong>Classifying Locations</strong>: Identify the type of place (e.g., tourism, university) using the response data.</li>
  <li><strong>Practical Example</strong>: Geocode "IIT Madras" and retrieve its full address, type (university), and other relevant information.</li>
  </ul>
  <p>Here are links and references:</p>
  <ul>
  <li><a href="https://colab.research.google.com/drive/1-vvP-UyMjHgBqc-hdsUhm3Bsbgi7oO6g" target="_blank" rel="noopener noreferrer">Geocoding using Nominatim - Notebook</a></li>
  <li>Learn about the <a href="https://geopy.readthedocs.io/" target="_blank" rel="noopener noreferrer"><code>geocoders</code> module in the <code>geopy</code> package</a></li>
  <li>Learn about the <a href="https://nominatim.org/release-docs/develop/api/Overview/" target="_blank" rel="noopener noreferrer"><code>nominatim</code> package</a></li>
  <li>If you get a HTTP Error 403 from Nominatim, use your email ID or your name instead of "myGeocoder" in <code>Nominatim(user_agent="myGeocoder")</code></li>
  </ul>
  </div><!---->
              <div class="card-body"><!--?lit$958752707$-->
      <div class="mb-3">
        <h2>Geospatial Data Optimization for UrbanRide</h2>
        <p>
          <strong>UrbanRide</strong> is a leading transportation and logistics company operating in major metropolitan
          areas worldwide. To enhance their service efficiency, optimize route planning, and improve customer
          satisfaction, UrbanRide relies heavily on accurate geospatial data. Precise bounding box information of cities
          helps in defining service zones, managing fleet distribution, and analyzing regional demand patterns.
        </p>
        <p>
          As UrbanRide expands into new cities, the company faces the challenge of accurately delineating service areas
          within these urban environments. Defining the geographical boundaries of a city is crucial for:
        </p>
        <ul>
          <li>
            <strong>Route Optimization:</strong> Ensuring drivers operate within designated zones to minimize transit
            times and fuel consumption.
          </li>
          <li>
            <strong>Fleet Management:</strong> Allocating vehicles effectively across different regions based on demand
            and service coverage.
          </li>
          <li>
            <strong>Market Analysis:</strong> Understanding regional demand to tailor services and promotional efforts
            accordingly.
          </li>
        </ul>
        <p>
          However, manually extracting and verifying bounding box data for each city is time-consuming and prone to
          inconsistencies, especially when dealing with cities that share names across different countries or have
          multiple administrative districts.
        </p>
        <p>
          UrbanRide’s data analytics team needs to automate the extraction of precise bounding box coordinates
          (specifically the minimum and maximum latitude) for various populous cities across different countries. This
          automation ensures consistency, accuracy, and scalability as the company grows its operations.
        </p>
        <p>
          To achieve this, the team utilizes the <strong>Nominatim API</strong>, a geocoding service based on
          OpenStreetMap data, to programmatically retrieve geospatial information. However, challenges arise when cities
          with the same name exist in multiple countries or have multiple entries within the same country. To address
          this, the team must implement a method to select the correct city instance based on specific identifiers (e.g.,
          <code>osm_id</code> patterns).
        </p>
        <h2>Your Task</h2>
        <p>What is the <!--?lit$958752707$-->maximum latitude of the bounding box of the city <!--?lit$958752707$-->Wuhan in the country <!--?lit$958752707$-->China on the Nominatim API?</p>
        <ol>
          <li>
            <strong>API Integration:</strong> Use the Nominatim API to fetch geospatial data for a specified city within a
            country via a GET request to the Nominatim API with parameters for the city and country. Ensure adherence to
            Nominatim’s usage policies, including rate limiting and proper attribution.
          </li>
          <li>
            <strong>Data Retrieval and Filtering:</strong> Parse the JSON response from the API. If multiple results are
            returned (e.g., multiple cities named “Springfield” in different states), filter the results based on the
            provided <code>osm_id</code> ending to select the correct city instance.
          </li>
          <li>
            <strong>Parameter Extraction:</strong> Access the <code>boundingbox</code> attribute. Depending on whether
            you're looking for the minimum or maximum latitude, extract the corresponding latitude value.
          </li>
        </ol>
        <h2>Impact</h2>
        <p>By automating the extraction and processing of bounding box data, UrbanRide can:</p>
        <ul>
          <li>
            <strong>Optimize Routing:</strong> Enhance route planning algorithms with precise geographical boundaries,
            reducing delivery times and operational costs.
          </li>
          <li>
            <strong>Improve Fleet Allocation:</strong> Allocate vehicles more effectively across defined service zones
            based on accurate city extents.
          </li>
          <li>
            <strong>Enhance Market Analysis:</strong> Gain deeper insights into regional performance, enabling targeted
            marketing and service improvements.
          </li>
          <li>
            <strong>Scale Operations:</strong> Seamlessly integrate new cities into their service network with minimal
            manual intervention, ensuring consistent data quality.
          </li>
        </ul>
  
        <label class="form-label" for="q-nominatim-api">
          What is the <!--?lit$958752707$-->maximum latitude of the bounding box of the city <!--?lit$958752707$-->Wuhan in the country <!--?lit$958752707$-->China on the Nominatim API?
        </label>
        <label class="form-label" for="q-nominatim-api">Value of the <!--?lit$958752707$-->maximum latitude</label>
        <input class="form-control" required="" id="q-nominatim-api" name="q-nominatim-api" disabled=""><div class="valid-feedback mb-3 comment">Correct!</div>
              <div class="invalid-feedback mb-3 comment">Incorrect. Try again.</div>
      </div>
    </div>
              <div class="card-footer d-flex">
                <button type="button" class="btn btn-primary check-answer" data-question="q-nominatim-api" disabled="">Check</button>
              </div>
            </div>
          <!----><!---->
            <div class="card my-5" data-question="q-hacker-news-search" id="hq-hacker-news-search">
              <div class="card-header">
                <span class="badge text-bg-primary me-2"><!--?lit$958752707$-->6</span>
                <!--?lit$958752707$-->Search Hacker News (<!--?lit$958752707$-->1 <!--?lit$958752707$-->mark)
              </div>
              <!--?lit$958752707$-->
              <div class="card-body"><!--?lit$958752707$-->
      <div class="mb-3">
        <h2>Media Intelligence for TechInsight Analytics</h2>
        <p>
          <strong>TechInsight Analytics</strong> is a leading market research firm specializing in technology trends and
          media intelligence. The company provides actionable insights to tech companies, startups, and investors by
          analyzing online discussions, news articles, and social media posts. One of their key data sources is
          <strong>Hacker News</strong>, a popular platform where tech enthusiasts and professionals share and discuss the
          latest in technology, startups, and innovation.
        </p>
        <p>
          In the rapidly evolving tech landscape, staying updated with the latest trends and public sentiments is crucial
          for TechInsight Analytics' clients. Manual monitoring of Hacker News posts for specific topics and
          engagement levels is inefficient and error-prone due to the high volume of daily posts. To address this,
          TechInsight seeks to automate the process of identifying and extracting relevant Hacker News posts that mention
          specific technology topics and have garnered significant attention (measured by points).
        </p>
        <p>
          TechInsight Analytics has developed an internal tool that leverages the
          <a href="https://hnrss.github.io/">HNRSS API</a> to fetch the latest Hacker News posts. The tool needs to
          perform the following tasks:
        </p>
        <ol>
          <li>
            <strong>Topic Monitoring:</strong> Continuously monitor Hacker News for posts related to specific technology
            topics, such as "Artificial Intelligence," "Blockchain," or "Cybersecurity."
          </li>
          <li>
            <strong>Engagement Filtering:</strong> Identify posts that have received a minimum number of points (votes) to
            ensure the content is highly engaging and relevant.
          </li>
          <li>
            <strong>Data Extraction:</strong> Extract essential details from the qualifying posts, including the
            post's link for further analysis and reporting.
          </li>
        </ol>
        <p>To achieve this, the team needs to create a program that:</p>
        <ul>
          <li>Searches Hacker News for the latest posts mentioning a specified topic.</li>
          <li>Filters these posts based on a minimum points threshold.</li>
          <li>Retrieves and returns the link to the most relevant post.</li>
        </ul>
        <h2>Your Task</h2>
        <p>
          Search using the <a href="https://hnrss.github.io/">Hacker News RSS API</a> for the latest Hacker News post
          mentioning <code><!--?lit$958752707$-->TypeScript</code> and having a minimum of <code><!--?lit$958752707$-->33</code> points. What is the link that it
          points to?
        </p>
        <ol>
          <li>
            <strong>Automate Data Retrieval:</strong> Utilize the HNRSS API to fetch the latest Hacker News posts. Use the
            URL relevant to fetching the latest posts, searching for topics and filtering by a minimum number of points.
          </li>
          <li>
            <strong>Extract and Present Data:</strong> Extract the most recent <code>&lt;item&gt;</code> from this result.
            Get the <code>&lt;link&gt;</code> tag inside it.
          </li>
          <li><strong>Share the result:</strong> Type in just the URL in the answer.</li>
        </ol>
  
        <label class="form-label" for="q-hacker-news-search">
          What is the link to the latest Hacker News post mentioning <code><!--?lit$958752707$-->TypeScript</code> having at least <!--?lit$958752707$-->33 points?
        </label>
        <input class="form-control" type="url" required="" id="q-hacker-news-search" name="q-hacker-news-search" disabled=""><div class="valid-feedback mb-3 comment">Correct!</div>
              <div class="invalid-feedback mb-3 comment">Incorrect. Try again.</div>
      </div>
    </div>
              <div class="card-footer d-flex">
                <button type="button" class="btn btn-primary check-answer" data-question="q-hacker-news-search" disabled="">Check</button>
              </div>
            </div>
          <!----><!---->
            <div class="card my-5" data-question="q-find-newest-github-user" id="hq-find-newest-github-user">
              <div class="card-header">
                <span class="badge text-bg-primary me-2"><!--?lit$958752707$-->7</span>
                <!--?lit$958752707$-->Find newest GitHub user (<!--?lit$958752707$-->1 <!--?lit$958752707$-->mark)
              </div>
              <!--?lit$958752707$-->
              <div class="card-body"><!--?lit$958752707$-->
      <h2>Emerging Developer Talent for CodeConnect</h2>
      <p>
        <strong>CodeConnect</strong> is an innovative recruitment platform that specializes in matching high-potential
        tech talent with forward-thinking companies. As the demand for skilled software developers grows, CodeConnect is
        committed to staying ahead of trends by leveraging data-driven insights to identify emerging developers—especially
        those who demonstrate strong community influence on platforms like GitHub.
      </p>
      <p>
        For CodeConnect, a key objective is to tap into regional talent pools to support local hiring initiatives and
        foster diversity within tech teams. One specific challenge is identifying developers in major tech hubs (such as
        Shanghai) who not only have established GitHub profiles but also show early signs of influence, as indicated by
        their follower counts.
      </p>
      <p>
        However, with millions of developers on GitHub and constantly evolving profiles, manually filtering through the
        data is impractical. CodeConnect needs an automated solution that:
      </p>
      <ol>
        <li>
          <strong>Filters Developer Profiles:</strong> Retrieves GitHub users based on location and a minimum follower
          threshold (e.g., over 60 followers) to focus on those with some level of social proof.
        </li>
        <li>
          <strong>Identifies the Newest Talent:</strong> Determines the most recent GitHub user in the selected group,
          providing insight into new emerging talent.
        </li>
        <li>
          <strong>Standardizes Data:</strong> Returns the account creation date in a standardized ISO 8601 format,
          ensuring consistent reporting across the organization.
        </li>
      </ol>
      <p>
        The recruitment team at CodeConnect is launching a new initiative aimed at hiring young, promising developers in
        Shanghai—a city known for its vibrant tech community. To support this initiative, the team has commissioned a
        project to use the GitHub API to find all users located in Shanghai with more than 60 followers. From this
        filtered list, they need to identify the newest account based on the profile creation date. This information will
        help the team target outreach efforts to developers who have recently joined the platform and may be eager to
        explore new career opportunities.
      </p>
      <h2>Your Task</h2>
      <p>
        Using the <a href="https://docs.github.com/en/rest">GitHub API</a>, find all users located in the city
        <code><!--?lit$958752707$-->Zurich</code> with over <code><!--?lit$958752707$-->50</code> followers.
      </p>
      <p>When was the newest user's GitHub profile created?</p>
      <ol>
        <li>
          <strong>API Integration and Data Retrieval:</strong> Leverage GitHub’s search endpoints to query users by
          location and filter them by follower count.
        </li>
        <li>
          <strong>Data Processing:</strong> From the returned list of GitHub users, isolate those profiles that meet the
          specified criteria.
        </li>
        <li>
          <strong>Sort and Format:</strong> Identify the "newest" user by comparing the
          <code>created_at</code> dates provided in the user profile data. Format the account creation date in the ISO
          8601 standard (e.g., "2024-01-01T00:00:00Z").
        </li>
      </ol>
      <h2>Impact</h2>
      <p>By automating this data retrieval and filtering process, CodeConnect gains several strategic advantages:</p>
      <ul>
        <li>
          <strong>Targeted Recruitment:</strong> Quickly identify new, promising talent in key regions, allowing for more
          focused and timely recruitment campaigns.
        </li>
        <li>
          <strong>Competitive Intelligence:</strong> Stay updated on emerging trends within local developer communities
          and adjust talent acquisition strategies accordingly.
        </li>
        <li>
          <strong>Efficiency:</strong> Automating repetitive data collection tasks frees up time for recruiters to focus
          on engagement and relationship-building.
        </li>
        <li>
          <strong>Data-Driven Decisions:</strong> Leverage standardized and reliable data to support strategic business
          decisions in recruitment and market research.
        </li>
      </ul>
      <div class="mb-3">
        <label class="form-label" for="q-find-newest-github-user">Enter the date (ISO 8601, e.g. "2024-01-01T00:00:00Z") when the newest user joined GitHub.</label>
        <input class="form-control" id="q-find-newest-github-user" name="q-find-newest-github-user" disabled=""><div class="valid-feedback mb-3 comment">Correct!</div>
              <div class="invalid-feedback mb-3 comment">Incorrect. Try again.</div>
        <p class="text-muted">
          Search using <code>location:</code> and <code>followers:</code> filters, sort by <code>joined</code> descending,
          fetch the first <code>url</code>, and enter the <code>created_at</code> field. Ignore ultra-new users who JUST
          joined, i.e. after <code><!--?lit$958752707$-->3/31/2025, 11:02:26 PM</code>.
        </p>
      </div>
    </div>
              <div class="card-footer d-flex">
                <button type="button" class="btn btn-primary check-answer" data-question="q-find-newest-github-user" disabled="">Check</button>
              </div>
            </div>
          <!----><!---->
            <div class="card my-5" data-question="q-scheduled-github-actions" id="hq-scheduled-github-actions">
              <div class="card-header">
                <span class="badge text-bg-primary me-2"><!--?lit$958752707$-->8</span>
                <!--?lit$958752707$-->Create a Scheduled GitHub Action (<!--?lit$958752707$-->1 <!--?lit$958752707$-->mark)
              </div>
              <!--?lit$958752707$--><!----><div class="card-body border-bottom"><!--?lit$958752707$--><h2>Scheduled Scraping with GitHub Actions</h2>
  <p>GitHub Actions provides an excellent platform for running web scrapers on a schedule. This tutorial shows how to automate data collection from websites using GitHub Actions workflows.</p>
  <h3>Key Concepts</h3>
  <ul>
  <li><strong>Scheduling</strong>: Use <a href="https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#schedule" target="_blank" rel="noopener noreferrer">cron syntax</a> to run scrapers at specific times</li>
  <li><strong>Dependencies</strong>: Install required packages like <code>httpx</code>, <code>lxml</code></li>
  <li><strong>Data Storage</strong>: Save scraped data to files and commit back to the repository</li>
  <li><strong>Error Handling</strong>: Implement robust error handling for network issues and HTML parsing</li>
  <li><strong>Rate Limiting</strong>: Respect website terms of service and implement delays between requests</li>
  </ul>
  <p>Here's a sample <code>scrape.py</code> that scrapes the IMDb Top 250 movies using httpx and lxml:</p>
  <pre><code class="language-python hljs" data-highlighted="yes"><span class="hljs-keyword">import</span> json
  <span class="hljs-keyword">import</span> httpx
  <span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime, UTC
  <span class="hljs-keyword">from</span> lxml <span class="hljs-keyword">import</span> html
  <span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">List</span>, <span class="hljs-type">Dict</span>
  
  
  <span class="hljs-keyword">def</span> <span class="hljs-title function_">scrape_imdb</span>() -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">str</span>]]:
      <span class="hljs-string">"""Scrape IMDb Top 250 movies using httpx and lxml.
  
      Returns:
          List of dictionaries containing movie title, year, and rating.
      """</span>
      headers = {<span class="hljs-string">"User-Agent"</span>: <span class="hljs-string">"Mozilla/5.0 (compatible; IMDbBot/1.0)"</span>}
      response = httpx.get(<span class="hljs-string">"https://www.imdb.com/chart/top/"</span>, headers=headers)
      response.raise_for_status()
  
      tree = html.fromstring(response.text)
      movies = []
  
      <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> tree.cssselect(<span class="hljs-string">".ipc-metadata-list-summary-item"</span>):
          title = (
              item.cssselect(<span class="hljs-string">".ipc-title__text"</span>)[<span class="hljs-number">0</span>].text_content()
              <span class="hljs-keyword">if</span> item.cssselect(<span class="hljs-string">".ipc-title__text"</span>)
              <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>
          )
          year = (
              item.cssselect(<span class="hljs-string">".cli-title-metadata span"</span>)[<span class="hljs-number">0</span>].text_content()
              <span class="hljs-keyword">if</span> item.cssselect(<span class="hljs-string">".cli-title-metadata span"</span>)
              <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>
          )
          rating = (
              item.cssselect(<span class="hljs-string">".ipc-rating-star"</span>)[<span class="hljs-number">0</span>].text_content()
              <span class="hljs-keyword">if</span> item.cssselect(<span class="hljs-string">".ipc-rating-star"</span>)
              <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>
          )
  
          <span class="hljs-keyword">if</span> title <span class="hljs-keyword">and</span> year <span class="hljs-keyword">and</span> rating:
              movies.append({<span class="hljs-string">"title"</span>: title, <span class="hljs-string">"year"</span>: year, <span class="hljs-string">"rating"</span>: rating})
  
      <span class="hljs-keyword">return</span> movies
  
  
  <span class="hljs-comment"># Scrape data and save with timestamp</span>
  now = datetime.now(UTC)
  <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">f'imdb-top250-<span class="hljs-subst">{now.strftime(<span class="hljs-string">"%Y-%m-%d"</span>)}</span>.json'</span>, <span class="hljs-string">"a"</span>) <span class="hljs-keyword">as</span> f:
      f.write(json.dumps({<span class="hljs-string">"timestamp"</span>: now.isoformat(), <span class="hljs-string">"movies"</span>: scrape_imdb()}) + <span class="hljs-string">"\n"</span>)
  </code></pre>
  <p>Here's a sample <code>.github/workflows/imdb-top250.yml</code> that runs the scraper daily and saves the data:</p>
  <pre><code class="language-yaml hljs" data-highlighted="yes"><span class="hljs-attr">name:</span> <span class="hljs-string">Scrape</span> <span class="hljs-string">IMDb</span> <span class="hljs-string">Top</span> <span class="hljs-number">250</span>
  
  <span class="hljs-attr">on:</span>
    <span class="hljs-attr">schedule:</span>
      <span class="hljs-comment"># Runs at 00:00 UTC every day</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">cron:</span> <span class="hljs-string">"0 0 * * *"</span>
    <span class="hljs-attr">workflow_dispatch:</span> <span class="hljs-comment"># Allow manual triggers</span>
  
  <span class="hljs-attr">jobs:</span>
    <span class="hljs-attr">scrape-imdb:</span>
      <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
      <span class="hljs-attr">permissions:</span>
        <span class="hljs-attr">contents:</span> <span class="hljs-string">write</span>
  
      <span class="hljs-attr">steps:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Checkout</span> <span class="hljs-string">repository</span>
          <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@v4</span>
  
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Install</span> <span class="hljs-string">uv</span>
          <span class="hljs-attr">uses:</span> <span class="hljs-string">astral-sh/setup-uv@v5</span>
  
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Run</span> <span class="hljs-string">scraper</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|</span> <span class="hljs-comment"># python</span>
            <span class="hljs-string">uv</span> <span class="hljs-string">run</span> <span class="hljs-string">--with</span> <span class="hljs-string">httpx,lxml,cssselect</span> <span class="hljs-string">python</span> <span class="hljs-string">scrape.py</span>
  
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Commit</span> <span class="hljs-string">and</span> <span class="hljs-string">push</span> <span class="hljs-string">changes</span>
          <span class="hljs-attr">run:</span> <span class="hljs-string">|
            git config --local user.email "github-actions[bot]@users.noreply.github.com"
            git config --local user.name "github-actions[bot]"
            git add *.json
            git commit -m "Update IMDb Top 250 data [skip ci]" || exit 0
            git push
  </span></code></pre>
  <h3>Best Practices</h3>
  <ol>
  <li><strong>Cache Dependencies</strong>: Use GitHub's caching to speed up package installation</li>
  <li><strong>Handle Errors</strong>: Implement retries and error logging</li>
  <li><strong>Rate Limiting</strong>: Add delays between requests to avoid overwhelming servers</li>
  <li><strong>Data Validation</strong>: Verify scraped data structure before saving</li>
  <li><strong>Monitoring</strong>: Set up notifications for workflow failures</li>
  </ol>
  <h3>Tools and Resources</h3>
  <ul>
  <li><a href="https://www.python-httpx.org/" target="_blank" rel="noopener noreferrer">httpx</a>: Async HTTP client</li>
  <li><a href="https://github.com/marketplace?type=actions" target="_blank" rel="noopener noreferrer">GitHub Actions Marketplace</a></li>
  <li><a href="https://docs.github.com/en/actions" target="_blank" rel="noopener noreferrer">GitHub Actions Documentation</a></li>
  </ul>
  <h3>Video Tutorials</h3>
  <p><a href="https://youtu.be/eJG86J200nM" target="_blank" rel="noopener noreferrer"><img src="https://i.ytimg.com/vi_webp/eJG86J200nM/sddefault.webp" alt="How to run Github Actions on a Schedule" class="img-fluid"></a></p>
  </div><!---->
              <div class="card-body"><!--?lit$958752707$-->
      <div class="mb-3">
        <h2>Automating Repository Updates for DevSync</h2>
        <p>
          <strong>DevSync Solutions</strong> is a mid-sized software development company specializing in collaborative
          tools for remote teams. With a growing client base and an expanding portfolio of projects, DevSync emphasizes
          efficient workflow management and robust version control practices to maintain high-quality software delivery.
        </p>
        <p>
          As part of their commitment to maintaining seamless and transparent development processes, DevSync has
          identified the need to implement automated daily updates to their GitHub repositories. These updates serve
          multiple purposes:
        </p>
        <ol>
          <li>
            <strong>Activity Tracking:</strong> Ensuring that each repository reflects daily activity helps in monitoring
            project progress and team engagement.
          </li>
          <li>
            <strong>Automated Documentation:</strong> Regular commits can be used to update status files, logs, or
            documentation without manual intervention.
          </li>
          <li>
            <strong>Backup and Recovery:</strong> Automated commits provide an additional layer of backup, ensuring that
            changes are consistently recorded.
          </li>
          <li>
            <strong>Compliance and Auditing:</strong> Maintaining a clear commit history aids in compliance with industry
            standards and facilitates auditing processes.
          </li>
        </ol>
        <p>
          Manually managing these daily commits is inefficient and prone to human error, especially as the number of
          repositories grows. To address this, DevSync seeks to automate the process using GitHub Actions, ensuring
          consistency, reliability, and scalability across all projects.
        </p>
        <p>
          DevSync's DevOps team has decided to standardize the implementation of GitHub Actions across all company
          repositories. The objective is to create a scheduled workflow that runs once daily, adds a commit to the
          repository, and ensures that these actions are consistently tracked and verifiable.
        </p>
        <p>
          As a junior developer or DevOps engineer at DevSync, you are tasked with setting up this automation for a
          specific repository. This exercise will not only enhance your understanding of GitHub Actions but also
          contribute to the company's streamlined workflow management.
        </p>
  
        <h2>Your Task</h2>
        <p>
          Create a scheduled <a href="https://github.com/features/actions">GitHub action</a> that runs daily and adds a
          commit to your repository. The workflow should:
        </p>
  
        <ul>
          <li>
            Use <code>schedule</code> with <code>cron</code> syntax to run <strong>once per day</strong> (must use
            specific hours/minutes, not wildcards)
          </li>
          <li>Include a step with your email <code><!--?lit$958752707$-->22f2001103@ds.study.iitm.ac.in</code> in its name</li>
          <li>Create a commit in each run</li>
          <li>Be located in <code>.github/workflows/</code> directory</li>
        </ul>
        <p>After creating the workflow:</p>
        <ul>
          <li>Trigger the workflow and wait for it to complete</li>
          <li>Ensure it appears as the <strong>most recent action</strong> in your repository</li>
          <li>Verify that it creates a commit during or within 5 minutes of the workflow run</li>
        </ul>
        <p>
          <label class="form-label" for="q-scheduled-github-actions">
            Enter your repository URL (format: <code>https://github.com/USER/REPO</code>):
          </label>
          <input class="form-control" id="q-scheduled-github-actions" name="q-scheduled-github-actions" disabled=""><div class="valid-feedback mb-3 comment">Correct!</div>
              <div class="invalid-feedback mb-3 comment">Incorrect. Try again.</div>
        </p>
      </div>
    </div>
              <div class="card-footer d-flex">
                <button type="button" class="btn btn-primary check-answer" data-question="q-scheduled-github-actions" disabled="">Check</button>
              </div>
            </div>
          <!----><!---->
            <div class="card my-5" data-question="q-extract-tables-from-pdf" id="hq-extract-tables-from-pdf">
              <div class="card-header">
                <span class="badge text-bg-primary me-2"><!--?lit$958752707$-->9</span>
                <!--?lit$958752707$-->Extract tables from PDF (<!--?lit$958752707$-->1 <!--?lit$958752707$-->mark)
              </div>
              <!--?lit$958752707$--><!----><div class="card-body border-bottom"><!--?lit$958752707$--><h2>Scraping PDFs with Tabula</h2>
  <p><a href="https://youtu.be/yDoKlKyxClQ" target="_blank" rel="noopener noreferrer"><img src="https://i.ytimg.com/vi_webp/yDoKlKyxClQ/sddefault.webp" alt="Scrape PDFs with Tabula Python library" class="img-fluid"></a></p>
  <p>You'll learn how to scrape tables from PDFs using the <code>tabula</code> Python library, covering:</p>
  <ul>
  <li><strong>Import Libraries</strong>: Use Beautiful Soup for URL parsing and Tabula for extracting tables from PDFs.</li>
  <li><strong>Specify Save Location</strong>: Mount Google Drive to save scraped PDFs.</li>
  <li><strong>Identify PDF URLs</strong>: Parse the given URL to identify and select all PDF links.</li>
  <li><strong>Download PDFs</strong>: Loop through identified links, saving each PDF to the specified location.</li>
  <li><strong>Extract Tables</strong>: Use Tabula to read tabular content from the downloaded PDFs.</li>
  <li><strong>Control Extraction Area</strong>: Specify page and area coordinates to accurately extract tables, avoiding extraneous text.</li>
  <li><strong>Save Extracted Data</strong>: Convert the extracted table data into structured formats like CSV for further analysis.</li>
  </ul>
  <p>Here are links and references:</p>
  <ul>
  <li><a href="https://colab.research.google.com/drive/102Fv2Ji0J4mvao3mCse52E7Th8bZiuyf" target="_blank" rel="noopener noreferrer">PDF Scraping - Notebook</a></li>
  <li>Learn about the <a href="https://tabula-py.readthedocs.io/en/latest/tabula.html" target="_blank" rel="noopener noreferrer"><code>tabula</code> package</a></li>
  <li>Learn about the <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html" target="_blank" rel="noopener noreferrer"><code>pandas</code> package</a>. <a href="https://youtu.be/vmEHCJofslg" target="_blank" rel="noopener noreferrer">Video</a></li>
  </ul>
  </div><!---->
              <div class="card-body"><!--?lit$958752707$-->
      <div class="mb-3">
        <h2>Academic Performance Analysis for EduAnalytics</h2>
        <p>
          <strong>EduAnalytics Corp.</strong> is a leading educational technology company that partners with schools and
          educational institutions to provide data-driven insights into student performance. By leveraging advanced
          analytics and reporting tools, EduAnalytics helps educators identify trends, improve teaching strategies, and
          enhance overall student outcomes. One of their key offerings is the
          <strong>Performance Insight Dashboard</strong>, which aggregates and analyzes student marks across various
          subjects and demographic groups.
        </p>
        <p>
          EduAnalytics has recently onboarded <strong>Greenwood High School</strong>, a large educational institution
          aiming to optimize its teaching methods and improve student performance in core subjects. Greenwood High School
          conducts annual assessments in multiple subjects, and the results are compiled into detailed PDF reports each
          semester. However, manually extracting and analyzing this data is time-consuming and prone to errors, especially
          given the volume of data and the need for timely insights.
        </p>
        <p>
          To address this, EduAnalytics plans to automate the data extraction and analysis process, enabling Greenwood
          High School to receive precise and actionable reports without the delays associated with manual processing.
        </p>
        <p>
          As part of this initiative, you are a data analyst at EduAnalytics assigned to develop a module that processes
          PDF reports containing student marks. Each PDF, named in the format <code>xxx.pdf</code>, includes a
          comprehensive table listing student performances across various subjects, along with their respective groups.
        </p>
        <p><strong>Greenwood High School</strong> has specific analytical needs, such as:</p>
        <ul>
          <li>
            <strong>Subject Performance Analysis:</strong> Understanding how students perform in different subjects to
            identify areas needing improvement.
          </li>
          <li>
            <strong>Group-Based Insights:</strong> Analyzing performance across different student groups to ensure
            equitable educational support.
          </li>
          <li>
            <strong>Threshold-Based Reporting:</strong> Focusing on students who meet or exceed certain performance
            thresholds to tailor advanced programs or interventions.
          </li>
        </ul>
        <h2>Your Task</h2>
        <p>
          This file,
          <button class="btn btn-sm btn-outline-primary" type="button">
            <!--?lit$958752707$-->q-extract-tables-from-pdf.pdf
          </button>
          contains a table of student marks in Maths, Physics, English, Economics, and Biology.
        </p>
        <p>
          Calculate the total <code><!--?lit$958752707$-->Economics</code> marks of students who scored <code><!--?lit$958752707$-->12</code> or more marks in
          <code><!--?lit$958752707$-->Maths</code> in groups <code><!--?lit$958752707$-->1-<!--?lit$958752707$-->36</code> (including both groups).
        </p>
        <ol>
          <li>
            <strong>Data Extraction:</strong>: Retrieve the PDF file containing the student marks table and use PDF
            parsing libraries (e.g., <code>Tabula</code>, <code>Camelot</code>, or <code>PyPDF2</code>) to accurately
            extract the table data into a workable format (e.g., CSV, Excel, or a DataFrame).
          </li>
          <li>
            <strong>Data Cleaning and Preparation:</strong> Convert marks to numerical data types to facilitate accurate
            calculations.
          </li>
          <li>
            <strong>Data Filtering:</strong> Identify students who have scored marks between <code><!--?lit$958752707$-->12</code> and
            <code><!--?lit$958752707$-->Maths</code> in groups <code><!--?lit$958752707$-->1-<!--?lit$958752707$-->36</code> (including both groups).
          </li>
          <li>
            <strong>Calculation:</strong> Sum the marks of the filtered students to obtain the total marks for this
            specific cohort.
          </li>
        </ol>
        <p>
          By automating the extraction and analysis of student marks, EduAnalytics empowers Greenwood High School to make
          informed decisions swiftly. This capability enables the school to:
        </p>
        <ul>
          <li>
            <strong>Identify Performance Trends:</strong> Quickly spot areas where students excel or need additional
            support.
          </li>
          <li>
            <strong>Allocate Resources Effectively:</strong> Direct teaching resources and interventions to groups and
            subjects that require attention.
          </li>
          <li>
            <strong>Enhance Reporting Efficiency:</strong> Reduce the time and effort spent on manual data processing,
            allowing educators to focus more on teaching and student engagement.
          </li>
          <li>
            <strong>Support Data-Driven Strategies:</strong> Use accurate and timely data to shape educational strategies
            and improve overall student outcomes.
          </li>
        </ul>
  
        <label class="form-label" for="q-extract-tables-from-pdf">
          What is the total <code><!--?lit$958752707$-->Economics</code> marks of students who scored <code><!--?lit$958752707$-->12</code> or more marks in
          <code><!--?lit$958752707$-->Maths</code> in groups <code><!--?lit$958752707$-->1-<!--?lit$958752707$-->36</code> (including both groups)?
        </label>
        <input class="form-control" type="number" required="" id="q-extract-tables-from-pdf" name="q-extract-tables-from-pdf" disabled=""><div class="valid-feedback mb-3 comment">Correct!</div>
              <div class="invalid-feedback mb-3 comment">Incorrect. Try again.</div>
      </div>
    </div>
              <div class="card-footer d-flex">
                <button type="button" class="btn btn-primary check-answer" data-question="q-extract-tables-from-pdf" disabled="">Check</button>
              </div>
            </div>
          <!----><!---->
            <div class="card my-5" data-question="q-pdf-to-markdown" id="hq-pdf-to-markdown">
              <div class="card-header">
                <span class="badge text-bg-primary me-2"><!--?lit$958752707$-->10</span>
                <!--?lit$958752707$-->Convert a PDF to Markdown (<!--?lit$958752707$-->1 <!--?lit$958752707$-->mark)
              </div>
              <!--?lit$958752707$--><!----><div class="card-body border-bottom"><!--?lit$958752707$--><h2>Convert PDFs to Markdown</h2>
  <ul>
  <li><a href="https://pymupdf.readthedocs.io/" target="_blank" rel="noopener noreferrer">PyMuPDF</a> is emerging as a strong default for PDF text extraction.</li>
  <li><a href="https://pymupdf.readthedocs.io/en/latest/pymupdf4llm/index.html" target="_blank" rel="noopener noreferrer">PyMuPDF4LLM</a> is apart of PyMuPDF that generates Markdown from PDFs that's suitable for LLMs.</li>
  <li><a href="https://unstructured.io/" target="_blank" rel="noopener noreferrer">unstructured</a> is radidly becoming the de facto library for parsing over 40 different file types and extracting text and tables. It's particularly useful for generating content to pass to LLMs.</li>
  <li>markitdown</li>
  <li>docling</li>
  </ul>
  </div><!---->
              <div class="card-body"><!--?lit$958752707$-->
      <div class="mb-3">
        <h2 id="digital-documentation-transformation-for-edudocs-inc-">
          Digital Documentation Transformation for EduDocs Inc.
        </h2>
        <p>
          <strong>EduDocs Inc.</strong> is a leading provider of educational resources and documentation management
          solutions for academic institutions. With a growing client base comprising universities, colleges, and online
          learning platforms, EduDocs emphasizes the importance of accessible, well-formatted digital documentation. To
          maintain high standards and streamline their content delivery, EduDocs continually seeks to enhance its
          documentation workflows and ensure consistency across all materials.
        </p>
        <p>
          EduDocs manages a vast repository of educational materials, including course syllabi, lecture notes, research
          papers, and administrative documents. These materials are often provided by clients in various formats,
          predominantly PDF, which poses challenges for content reuse, editing, and integration into digital platforms.
        </p>
        <p>
          Manually converting PDF documents to Markdown is time-consuming and prone to errors, especially when dealing
          with large volumes of documents. Additionally, ensuring that the converted Markdown adheres to consistent
          formatting standards is crucial for maintaining the professional quality of EduDocs' deliverables.
        </p>
        <p>
          To address these challenges, EduDocs aims to automate and standardize the conversion of PDF documents to
          Markdown format, ensuring that all Markdown files are consistently formatted using Prettier 3.4.2. This
          initiative will improve efficiency, reduce manual effort, and enhance the overall quality of the documentation
          provided to clients.
        </p>
        <h2>Your Task</h2>
        <p>
          As part of the <strong>Documentation Transformation Project</strong>, you are a junior developer at EduDocs
          tasked with developing a streamlined workflow for converting PDF files to Markdown and ensuring their consistent
          formatting. This project is critical for supporting EduDocs' commitment to delivering high-quality,
          accessible educational resources to its clients.
        </p>
        <p>
          <button class="btn btn-sm btn-outline-primary" type="button">
            <!--?lit$958752707$-->q-pdf-to-markdown.pdf
          </button>
          has the contents of a sample document.
        </p>
        <ol>
          <li>
            <strong>Convert the PDF to Markdown:</strong> Extract the content from the PDF file. Accurately convert the
            extracted content into Markdown format, preserving the structure and formatting as much as possible.
          </li>
          <li><strong>Format the Markdown:</strong> Use Prettier version 3.4.2 to format the converted Markdown file.</li>
          <li>
            <strong>Submit the Formatted Markdown:</strong> Provide the final, formatted Markdown file as your submission.
          </li>
        </ol>
        <h2>Impact</h2>
        <p>
          By completing this exercise, you will contribute to EduDocs Inc.'s mission of providing high-quality,
          accessible educational resources. Automating the PDF to Markdown conversion and ensuring consistent formatting:
        </p>
        <ul>
          <li>
            <strong>Enhances Productivity:</strong> Reduces the time and effort required to prepare documentation for
            clients.
          </li>
          <li>
            <strong>Improves Quality:</strong> Ensures all documents adhere to standardized formatting, enhancing
            readability and professionalism.
          </li>
          <li>
            <strong>Supports Scalability:</strong> Enables EduDocs to handle larger volumes of documentation without
            compromising on quality.
          </li>
          <li>
            <strong>Facilitates Integration:</strong> Makes it easier to integrate Markdown-formatted documents into
            various digital platforms and content management systems.
          </li>
        </ul>
  
        <label class="form-label" for="q-pdf-to-markdown">
          What is the markdown content of the PDF, formatted with prettier@3.4.2?
        </label>
        <textarea rows="8" class="form-control" required="" id="q-pdf-to-markdown" name="q-pdf-to-markdown" disabled=""></textarea><div class="valid-feedback mb-3 comment">Correct!</div>
              <div class="invalid-feedback mb-3 comment">Incorrect. Try again.</div>
        <p class="text-muted">
          It is <em>very hard</em> to get the correct Markdown output from a PDF. Any method you use will likely require
          manual corrections. To make it easy, this question only checks a few basic things.
        </p>
      </div>
    </div>
              <div class="card-footer d-flex">
                <button type="button" class="btn btn-primary check-answer" data-question="q-pdf-to-markdown" disabled="">Check</button>
              </div>
            </div>
          <!----></section>
        <button type="submit" class="btn btn-success check-action" title="Check your score" disabled="">Check all</button>
        <button type="button" class="btn btn-primary save-action" title="Save your progress" disabled="">Save</button>
        <div id="submission-status" class="my-3"></div>
        <p>Save regularly. Your <em>last saved</em> submission will be evaluated.</p>
      </form>
  
      <footer class="my-5 d-flex align-items-center justify-content-center" style="height: 50vh;">
        <h1 class="display-4">Best of luck!</h1>
      </footer>
    </main>
  
    <script type="module" src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@gramex/ui@0.3/dist/dark-theme.js" integrity="sha384-WWZYgp4BxSnWElq/A+SNJoNuJ5eZf6xBLqGUBHmu6QXN+9dqtz7bTtyuBwy9O7C+" crossorigin="anonymous"></script>
    <script src="https://accounts.google.com/gsi/client"></script>
    <script type="module">
      import { setup } from './exam.js';
      setup(window.location.pathname.slice(1));
    </script>
  
  
  
  </body></html>